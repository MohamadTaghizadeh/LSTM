{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMhMgE5ZZXwHq0iIX/ZEWUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamadTaghizadeh/LSTM/blob/main/LSTM_sales_forecasting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##introductories"
      ],
      "metadata": {
        "id": "gMeIPMKNn-NT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dr5_eVTan15o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from numpy import hstack\n",
        "import pandas as pd\n",
        "import time \n",
        "import datetime as dt\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from itertools import cycle\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.sparse import csr_matrix\n",
        "from tensorflow import keras\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model, Sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, LSTM, Dropout\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pandas and pyplot"
      ],
      "metadata": {
        "id": "1UOr6Sh_oGju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up Bayesian Methods for Hackers matplotlib style\n",
        "plt.style.use('bmh')\n",
        "\n",
        "# Set up custom color palette/optional\n",
        "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])"
      ],
      "metadata": {
        "id": "sm-HCLSHn7Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##loading data and reduce the memory usage"
      ],
      "metadata": {
        "id": "KHd2QqduoM9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Reduces memory ocuppied by a dataframe by transfroming column type to the smallest type \n",
        "with capacity enough to store the column content\n",
        "Parameters:\n",
        "    df - a dataframe to reduce memory usage\n",
        "    verbose - a boolen flag which indicates if infromation about memory reduction is printed\n",
        "Returns:\n",
        "    A data frame transformed to use as little memory as possible\n",
        "\"\"\"\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df"
      ],
      "metadata": {
        "id": "4C0Er3I8n7Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates dataframes with reduced memory usage from competition .csv files \n",
        "Parameters:\n",
        "    input_dir - a path to M5 dataset\n",
        "    verbose - a boolen flag which indicates if infromation about memory reduction is printed\n",
        "Returns:\n",
        "    sell_prices_df - a dataframe containing sell_prices.csv data\n",
        "    calendar_df - a dataframe containing calendar.csv data\n",
        "    sales_train_evaluation_df - a dataframe containing sales_train_evaluation.csv data\n",
        "    sample_submission - a dataframe containing sample_submission.csv data\n",
        "\"\"\"\n",
        "def read_data(input_dir):\n",
        "    sell_prices_df = reduce_mem_usage(pd.read_csv(input_dir + 'sell_prices.csv'))\n",
        "    print('Sell prices has {} rows and {} columns'.format(sell_prices_df.shape[0], sell_prices_df.shape[1]))\n",
        "\n",
        "    calendar_df = pd.read_csv(input_dir + 'calendar.csv')\n",
        "    # Cast string representation of data to datetime\n",
        "    # it will simplify creation of calendar features\n",
        "    calendar_df[\"date\"] = pd.to_datetime(calendar_df[\"date\"])\n",
        "    calendar_df = reduce_mem_usage(calendar_df)\n",
        "    print('Calendar has {} rows and {} columns'.format(calendar_df.shape[0], calendar_df.shape[1]))\n",
        "    \n",
        "    # Initially sales_train_validation.csv was used here, but after validation data was released we switched to sales_train_evaluation.csv\n",
        "    # to simplify comparison of model prediction to ground truth validation data\n",
        "    sales_train_evaluation_df = reduce_mem_usage(pd.read_csv(input_dir + 'sales_train_evaluation.csv'))\n",
        "    print('Sales train validation has {} rows and {} columns'.format(sales_train_evaluation_df.shape[0], sales_train_evaluation_df.shape[1]))\n",
        "\n",
        "    submission_df = reduce_mem_usage(pd.read_csv(input_dir + 'sample_submission.csv'))\n",
        "    return sell_prices_df, calendar_df, sales_train_evaluation_df, submission_df\n",
        "\n",
        "\"\"\"\n",
        "Creates a data frame for items sales per day with item ids as columns names and dates as the index  \n",
        "Parameters:\n",
        "    calendar_df - a dataframe containing calendar.csv data with 'date' in datetime format\n",
        "    sales_train_evaluation_df - a dataframe containing sales_train_evaluation.csv data\n",
        "Returns:\n",
        "    A dataframe containing sales for training and validation days\n",
        "    rows represent days and column a unique item id\n",
        "    index is set to dates in datetime format\n",
        "\"\"\"\n",
        "def create_sales_df(calendar_df, sales_train_evaluation_df):\n",
        "    # Create a unique item id from 'item_id' and 'store_id'\n",
        "    sales_train_evaluation_df['item_store_id'] = sales_train_evaluation_df.apply(\n",
        "        lambda x: x['item_id'] + '_' + x['store_id'], axis=1)\n",
        "    # Extract training and validation days\n",
        "    DF_Sales = sales_train_evaluation_df.loc[:, 'd_1':'d_1941'].T\n",
        "    # Set unique item ids as columns\n",
        "    DF_Sales.columns = sales_train_evaluation_df['item_store_id'].values\n",
        "    \n",
        "    # Set dates as index\n",
        "    date_index = calendar_df['date']\n",
        "    dates = date_index[0:1941].values\n",
        "    DF_Sales = DF_Sales.set_index([dates])\n",
        "    DF_Sales.index = pd.to_datetime(DF_Sales.index)\n",
        "    return DF_Sales"
      ],
      "metadata": {
        "id": "LRHtYJVen7a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Competition dataset\n",
        "INPUT_DIR_PATH = '../input/m5-forecasting-accuracy/'\n",
        "# WRMSE calculation dataset\n",
        "INPUT_DIR_PATH_WRMSE = '../input/m5-accuracy-wrmse/'"
      ],
      "metadata": {
        "id": "QPPZMT6In7d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load calendar and sales data\n",
        "_,  calendar_df, sales_train_evaluation_df, _ = read_data(INPUT_DIR_PATH)"
      ],
      "metadata": {
        "id": "7pYWoK34n7gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sales dataframe containing training and validation data\n",
        "DF_Sales = create_sales_df(calendar_df, sales_train_evaluation_df)\n",
        "# Split the dataframe into training and validation data\n",
        "# We need to make a prediction for 28 days\n",
        "forecast_horizon = 28\n",
        "DF_Sales_Train = DF_Sales.iloc[:-forecast_horizon,:]\n",
        "DF_Sales_Val = DF_Sales.iloc[-forecast_horizon:,:]"
      ],
      "metadata": {
        "id": "5qwVk-8sn7i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calendar features extraction"
      ],
      "metadata": {
        "id": "TXiiDsQwogIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Creates a data frame with calendar features\n",
        "Parameters:\n",
        "    calendar_df - a dataframe containing calendar.csv data with 'date' in datetime format\n",
        "Returns:\n",
        "    A dataframe containing features:  \n",
        "        event_name_1_cat - a categorical feature which denotes a type of an event\n",
        "        snap_CA - a boolean feature which denotes if it is a snap day in CA\n",
        "        snap_TX - a boolean feature which denotes if it is a snap day in TX\n",
        "        snap_WI - a boolean feature which denotes if it is a snap day in WI\n",
        "        wday - a categorical feature which denotes a day of week\n",
        "        week - a categorical feature which denotes a number of week\n",
        "        month - a categorical feature which denotes a month\n",
        "        mday - a categorical feature which denotes a day number in a month\n",
        "\"\"\"\n",
        "def create_features(calendar_df):\n",
        "    cal_features_columns = [\"date\", \"event_name_1\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]\n",
        "    cal_features = calendar_df[cal_features_columns].fillna(0)\n",
        "    cal_features[\"event_name_1\"] = cal_features[\"event_name_1\"].astype('category')\n",
        "    cal_features[\"event_name_1_cat\"] = cal_features[\"event_name_1\"].cat.codes\n",
        "    del cal_features[\"event_name_1\"]\n",
        "    \n",
        "    date_features = {\n",
        "        \"wday\": \"weekday\",\n",
        "        \"week\": \"weekofyear\",\n",
        "        \"month\": \"month\",\n",
        "        \"mday\": \"day\",\n",
        "    }\n",
        "\n",
        "    for date_feat_name, date_feat_func in date_features.items():\n",
        "        cal_features[date_feat_name] = getattr(cal_features[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
        "    \n",
        "    del cal_features[\"date\"]\n",
        "    \n",
        "    return cal_features"
      ],
      "metadata": {
        "id": "HY9z0qYXn7lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from calendar dataframe\n",
        "cal_features = create_features(calendar_df)\n",
        "# Split features into training a validation dataframes\n",
        "# The validation data dataframe will be used for features extraction during prediction\n",
        "cal_train = cal_features.iloc[:-forecast_horizon * 2,:]\n",
        "cal_val = cal_features.iloc[-forecast_horizon * 2:,:]"
      ],
      "metadata": {
        "id": "fBF6_p3kn7nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preparing the data for training step"
      ],
      "metadata": {
        "id": "HEKWLvGzonhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extracts a subset of data for training  \n",
        "Parameters:\n",
        "    data - a dataframe containing sales data in which rows represent days and columns - unique items\n",
        "    num_days_included - how many days to include into training set\n",
        "Returns:\n",
        "    A numpy array containing num_days_included last days from the dataset\n",
        "\"\"\"\n",
        "def train_subset(data, num_days_included=365):\n",
        "    all_data = np.array(data)\n",
        "    data_subset = all_data[-num_days_included:, :]\n",
        "    return data_subset"
      ],
      "metadata": {
        "id": "TdJV-WsTn7qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Applies MinMaxScaler on the data \n",
        "Parameters:\n",
        "    data - a numpy array containing sales data in which rows represent days and columns - unique items\n",
        "    scaler - a MinMaxScaler instance\n",
        "    plot - a boolean flag which indicates if we want to plot data distribution before and after scaling\n",
        "Returns:\n",
        "    A numpy array scaled by scaler\n",
        "\"\"\"\n",
        "def scaled_data(data, scaler, plot=False):\n",
        "    train_data_normalized = scaler.fit_transform(data)\n",
        "\n",
        "    if plot:\n",
        "        # we check that data distribution did not change after normalization\n",
        "        fig, axs = plt.subplots(2)\n",
        "        fig.suptitle('Data Distribution Before and After Normalization ', fontsize=19)\n",
        "        pd.DataFrame(data).plot(kind='hist', ax=axs[0], alpha=.4, figsize=[12, 6], legend=False,\n",
        "                                title=' Before Normalization', color='red')\n",
        "        pd.DataFrame(train_data_normalized).plot(kind='hist', ax=axs[1], figsize=[12, 6], alpha=.4, legend=False,\n",
        "                                                 title=' After Normalization' \\\n",
        "                                                 , color='blue')\n",
        "\n",
        "    return train_data_normalized"
      ],
      "metadata": {
        "id": "lUhuOMN7n7tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Splits the data into training and validation sets\n",
        "Parameters:\n",
        "    dataset - a numpy array containing sales data in which rows represent days and columns - unique items\n",
        "    validation_size - how many days to include into validation dataset\n",
        "Returns:\n",
        "    A tuple (train_data, val_data)\n",
        "    train_data - data to be used for model training\n",
        "    val_data - data to be used for validation. None if validation_size=0\n",
        "\"\"\"\n",
        "def train_val_split(dataset, validation_size=0):\n",
        "    if validation_size > 0:\n",
        "        train_size = len(dataset[:,0]) - validation_size\n",
        "\n",
        "        train = dataset[:train_size]\n",
        "        val = dataset[train_size:]\n",
        "        return train, val\n",
        "    else:\n",
        "        return dataset, None"
      ],
      "metadata": {
        "id": "1gZBUjsgn7v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the subset used for training\n",
        "data = train_subset(DF_Sales_Train, 1913)\n",
        "# Normalize to impove NN training\n",
        "scaler = MinMaxScaler()\n",
        "normalized_data = scaled_data(data, scaler)\n",
        "# Split the training set into training and validation parts\n",
        "# validation_data were used in early stage of model development, \n",
        "# now we use the whole training set to fit the model\n",
        "train_data, validation_data = train_val_split(normalized_data, 0)\n",
        "# Scale calendar features\n",
        "cal_data_train = cal_train.values\n",
        "scaler_cal = MinMaxScaler()\n",
        "normalized_cal = scaler_cal.fit_transform(cal_data_train)"
      ],
      "metadata": {
        "id": "-yXJrXOdouni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The LSTM model"
      ],
      "metadata": {
        "id": "94VceWr1ow9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Builds LTSM neural network\n",
        "Parameters:\n",
        "    input_shape - a tuple in which the first item is a sequence length\n",
        "                  and the second one is a number of features\n",
        "    output_num - a number of sequences to predict\n",
        "Returns:\n",
        "    A Sequential LSTM model \n",
        "\"\"\"\n",
        "def LSTM_model(input_shape, output_num):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adding the first LSTM layer and some dropout\n",
        "    layer1_units_num = 50\n",
        "    model.add(LSTM(units=layer1_units_num, return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Adding a second LSTM layer and dropout\n",
        "    layer2_units_num = 300\n",
        "    model.add(LSTM(units=layer2_units_num, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    # Adding a third LSTM layer and dropout\n",
        "    layer3_units_num = 300\n",
        "    model.add(LSTM(units=layer3_units_num))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    # Adding the output layer for prediction\n",
        "    model.add(Dense(units = output_num))\n",
        "    return model"
      ],
      "metadata": {
        "id": "WfSEm-j2ouqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 28\n",
        "# We add calendar features to sales data to train the model, thus number of features is sum\n",
        "# of time series number and number of calendar features\n",
        "features_number = train_data.shape[1] + normalized_cal.shape[1]\n",
        "lstm = LSTM_model((seq_length, features_number), 30490)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "id": "1TUgRAMLousv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(lstm, to_file='lstm.png')"
      ],
      "metadata": {
        "id": "MFGXPEXMo47v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##data generator"
      ],
      "metadata": {
        "id": "1ou__ttzo7ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data generator of training sequences and labels for LSTM\n",
        "\"\"\"\n",
        "class SlidingWindowDataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        timeseries - a numpy array with sales data (rows denote days and columns - products)\n",
        "        calendar - a numpy array with calendar features (rows denote days and columns - features)\n",
        "        train_seq_len - a length of training sequences\n",
        "        batch_size - a batch size\n",
        "        shuffle - a boolean flag which denotes if data is shuffled\n",
        "    \"\"\"\n",
        "    def __init__(self, timeseries, calendar, train_seq_len=28, batch_size=32, shuffle=True):\n",
        "        self.timeseries = timeseries\n",
        "        self.calendar = calendar\n",
        "        self.train_seq_len = train_seq_len\n",
        "        self.batch_size = batch_size\n",
        "        # Generally if we want to extract sequences with sliding window, the number of sequences\n",
        "        # = time series steps number - length of training sequence + 1\n",
        "        # however we also need 1 time step data as labes\n",
        "        self.seq_num = len(self.timeseries) - self.train_seq_len\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.n = 0\n",
        "        self.max = self.__len__()\n",
        "        \n",
        "    \"\"\"\n",
        "    Calculates the length of an object\n",
        "    Returns:\n",
        "        A number of batches per epoch\n",
        "    \"\"\"\n",
        "    def __len__(self):\n",
        "        \n",
        "        return int(np.floor(self.seq_num / self.batch_size))\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a next batch of sequences\n",
        "    Returns:\n",
        "        A next batch. Useful for object debuging\n",
        "    \"\"\"\n",
        "    def __next__(self):\n",
        "        if self.n >= self.max:\n",
        "            self.n = 0\n",
        "        result = self.__getitem__(self.n)\n",
        "        self.n += 1\n",
        "        return result\n",
        "\n",
        "    \"\"\"\n",
        "    Returns a batch of sequences at the specified index\n",
        "    Parameters:\n",
        "        index - an index of the requested batch\n",
        "    Returns:\n",
        "        A tuple of numpy arrays in which the first array contains a batch of training sequences\n",
        "        with shape [batch_size, sequence_len, features]\n",
        "        and the second array contains a batch of the corresponding labels\n",
        "        with shape [batch_size, output_labels]\n",
        "    \"\"\"\n",
        "    def __getitem__(self, index):\n",
        "        # Since indexes might be shuffled we need to obtain a set of shuffled indexes \n",
        "        # wich correspond to the specified index\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        x, y = [], []\n",
        "        for i in indexes:\n",
        "            # Extract sales data\n",
        "            x_next = self.timeseries[i:i+self.train_seq_len, :]\n",
        "            # Extract calendar features\n",
        "            calendar_next = self.calendar[i:i+self.train_seq_len, :]\n",
        "            # Join sales data and calendar features and add it to a batch of training data\n",
        "            x.append(hstack((x_next, calendar_next)))\n",
        "            # Add next time steps to a batch of labels\n",
        "            y_next = self.timeseries[i + self.train_seq_len, :]\n",
        "            y.append(y_next)\n",
        "        return np.array(x), np.array(y)\n",
        "    \n",
        "    \"\"\"\n",
        "    Shuffles indexes after each epoch if shuffle=True\n",
        "    \"\"\"\n",
        "    def on_epoch_end(self):\n",
        "        \n",
        "        self.indexes = np.arange(self.seq_num)\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "rV4IxNF5o491"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Traning of LSTM"
      ],
      "metadata": {
        "id": "_WufgO84pEn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with standart Adam optimizer\n",
        "lstm.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "batch_size = 32\n",
        "# Data generation for training\n",
        "data_gen_train = SlidingWindowDataGenerator(train_data, normalized_cal, train_seq_len=seq_length, batch_size=batch_size)\n",
        "# Data generation for validation. Only defined if validation_data is not None\n",
        "data_gen_validation = SlidingWindowDataGenerator(validation_data, normalized_cal, train_seq_len=seq_length, batch_size=1) if validation_data is not None else None\n",
        "\n",
        "# Fit LSTM model to data\n",
        "history = lstm.fit_generator(data_gen_train,\n",
        "                            epochs = 32, \n",
        "                            validation_data = data_gen_validation, \n",
        "                            steps_per_epoch= int(np.floor((len(train_data) - seq_length) / batch_size)))"
      ],
      "metadata": {
        "id": "qYSbkJ4Mo5AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a prediction"
      ],
      "metadata": {
        "id": "pdUs6lF7pL4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Performs a prediction for validation and evaluation data\n",
        "Parameters:\n",
        "    lstm - a trained lstm model\n",
        "    sequence_for_prediction - a sequence right befor the first validation day\n",
        "    calendar - normalized calendar features for validation set\n",
        "    scaler - a MinMaxScaler used to normalise sales data\n",
        "    prediction_window - how many validation and evaluation days\n",
        "    seq_length - a length of a sequence used in the model\n",
        "Returns:\n",
        "    A dataframe containing a prediction in which rows denote a day and column denote indexes of products.\n",
        "    Note that to match submission format same products for validation and evaluation sales \n",
        "    have different indexes. Thus shape of an array is 28x60980, not 56x30490\n",
        "\"\"\"\n",
        "def predict(lstm, sequence_for_prediction, calendar, scaler, prediction_window = 28, seq_length=28):\n",
        "    prediction_val = []\n",
        "    prediction_eval = []\n",
        "    \n",
        "    # Prediction for validation period\n",
        "    for i in range(0, prediction_window):\n",
        "        # Predict sales for the next day\n",
        "        prediction = lstm.predict(sequence_for_prediction)\n",
        "        prediction_val.append(prediction[0])\n",
        "        # Merge predicted sales and calendar features into last training sequence\n",
        "        next_seq = np.array([hstack((prediction[0], calendar[i]))])\n",
        "        # Add next sequence to the end of training sequences and drop the first one\n",
        "        sequence_for_prediction[0] = np.concatenate((sequence_for_prediction[0], next_seq), axis=0)[-seq_length:, :]\n",
        "\n",
        "    # Prediction for evaluation period\n",
        "    for i in range(0, prediction_window):\n",
        "        # Predict sales for the next day\n",
        "        prediction = lstm.predict(sequence_for_prediction)\n",
        "        prediction_eval.append(prediction[0])\n",
        "        # Merge predicted sales and calendar features into last training sequence\n",
        "        next_seq = np.array([hstack((prediction[0], calendar[prediction_window + i]))])\n",
        "        # Add next sequence to the end of training sequences and drop the first one\n",
        "        sequence_for_prediction[0] = np.concatenate((sequence_for_prediction[0], next_seq), axis=0)[-seq_length:, :]\n",
        "        \n",
        "    prediction_val = np.array(prediction_val)\n",
        "    prediction_eval = np.array(prediction_eval)\n",
        "\n",
        "    # Inverse normalize predictions\n",
        "    prediction_transformed_val = scaler.inverse_transform(prediction_val)\n",
        "    prediction_transformed_eval = scaler.inverse_transform(prediction_eval)\n",
        "    \n",
        "    # Merge validation and evaluation predictions\n",
        "    predictions = hstack((prediction_transformed_val, prediction_transformed_eval))\n",
        "    # NN often gives small negative output. This prediction is invalid, so we set all negative values to 0\n",
        "    predictions[predictions < 0] = 0\n",
        "\n",
        "    # create data frames\n",
        "    df_prediction = pd.DataFrame(predictions)\n",
        "    return df_prediction"
      ],
      "metadata": {
        "id": "Vmi5DMfgpGa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale calendar features for the validation set\n",
        "cal_val_scaled = scaler_cal.transform(cal_val.values)\n",
        "# Extract the sequence to predict the first validation day sales\n",
        "sequence_for_prediction = np.array([hstack((train_data[-seq_length:, :], cal_data_train[-seq_length:, :]))])\n",
        "# Predict sales and save it to dataframe\n",
        "df_prediction = predict(lstm, sequence_for_prediction, cal_val_scaled, scaler, 28, seq_length)"
      ],
      "metadata": {
        "id": "2a35aqzPpGdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##visualize the prediciton on the validation set"
      ],
      "metadata": {
        "id": "rbkbk8bipUQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plots a prediction and groud truth for a randomly selected product\n",
        "if groud truth is not specified plots only prediction\n",
        "Parameters:\n",
        "    df_predict - a numpy array with prediction (rows denote days and columns - products)\n",
        "    df_labels - a numpy array with ground truth (rows denote days and columns - products)\n",
        "\"\"\"\n",
        "def plot_random_prediction(prediction, labels=None):\n",
        "    # Plot prediction\n",
        "    figure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k') \n",
        "    index = random.randint(0, prediction.shape[1]-1)\n",
        "    plt.plot(prediction[:, index])\n",
        "    # If labels are present plot ground truth\n",
        "    if labels is not None:\n",
        "        plt.plot(labels[:, index])\n",
        "        plt.legend(['Prediction','Time Series'],fontsize = 21)\n",
        "    else:\n",
        "        plt.legend(['Prediction'],fontsize = 21)\n",
        "    plt.suptitle('Time-Series Prediction Test Set',fontsize = 23)\n",
        "    plt.xticks(fontsize=21)\n",
        "    plt.yticks(fontsize=21)\n",
        "    plt.ylabel(ylabel = 'Sales Demand',fontsize = 21)\n",
        "    plt.xlabel(xlabel = 'Date',fontsize = 21)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "atR-IFMJpGgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot prediction for a few series\n",
        "# Extract prediction for validation days\n",
        "prediction_val = np.array(df_prediction)[:, :30490]\n",
        "labels_val = np.array(DF_Sales_Val)\n",
        "for i in range(0, 10):\n",
        "    plot_random_prediction(prediction_val, labels_val)"
      ],
      "metadata": {
        "id": "zENYWJcjpakb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training loss\n",
        "plt.style.use('ggplot')\n",
        "plt.figure()\n",
        "epoch = len(history.history['loss'])\n",
        "plt.plot(np.arange(0, epoch), history.history['loss'], label='train_loss')\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nMwkvxxKpanc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##file file .csv format"
      ],
      "metadata": {
        "id": "vv5xXkAkpiCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Plots saves prediction to .csv file in competition submission format\n",
        "if groud truth is not specified plots only prediction\n",
        "Parameters:\n",
        "    df_prediction - a dataframe with prediction (rows denote days and columns - validation and evaluation products)\n",
        "    df_sales - a dataframe with sales data\n",
        "\"\"\"\n",
        "def create_submission(df_prediction, df_sales):\n",
        "    # Create columns names based on items ids in sales dataframe\n",
        "    columns_val = [c+\"_validation\" for c in df_sales.columns]\n",
        "    columns_eval = [c+\"_evaluation\" for c in df_sales.columns]\n",
        "    columns = columns_val + columns_eval\n",
        "    # Set columns as index\n",
        "    df_transposed = df_prediction.T\n",
        "    df_transposed = df_transposed.set_index([columns])\n",
        "    df_transposed.index.name = 'id'\n",
        "    # Set days number as colimns\n",
        "    days = [\"F1\", \"F2\", \"F3\", \"F4\", \"F5\", \"F6\", \"F7\", \"F8\", \"F9\", \"F10\", \"F11\", \"F12\", \"F13\", \"F14\", \n",
        "             \"F15\", \"F16\", \"F17\", \"F18\", \"F19\", \"F20\", \"F21\", \"F22\", \"F23\", \"F24\", \"F25\", \"F26\", \"F27\", \"F28\"]\n",
        "    df_transposed.columns = days\n",
        "    # Save to csv\n",
        "    df_transposed.to_csv(r'submission.csv')"
      ],
      "metadata": {
        "id": "jgJCr-_-pfVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write prediction into file\n",
        "create_submission(df_prediction, DF_Sales)"
      ],
      "metadata": {
        "id": "yTWNSEc-pfX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##WRMSE assesment"
      ],
      "metadata": {
        "id": "KEDNwHDqptz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function to do quick rollups\n",
        "Parameters:\n",
        "    roll_mat_csr - a sparse roll up matrix\n",
        "    v - np.array of size (30490 rows, n day columns)\n",
        "\"\"\"\n",
        "def rollup(roll_mat_csr, v):\n",
        "    return roll_mat_csr * v\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function to calculate WRMSSE\n",
        "Parameters:\n",
        "    preds - a dataframe of predictions (number of products rows, number of day columns)\n",
        "    y_true - a dataframe of groud truth (number of products rows, number of day columns)\n",
        "    roll_mat_csr - a sparse roll up matrix\n",
        "    s, w, sw - weights\n",
        "\"\"\"\n",
        "def wrmsse(preds, y_true, roll_mat_csr, s, w, sw, score_only=False):\n",
        "    if score_only:\n",
        "        return np.sum(\n",
        "                np.sqrt(\n",
        "                    np.mean(\n",
        "                        np.square(rollup(roll_mat_csr, preds - y_true))\n",
        "                            ,axis=1)) * sw)/12\n",
        "    else: \n",
        "        score_matrix = (np.square(rollup(roll_mat_csr, preds - y_true)) * np.square(w)[:, None])/ s[:, None]\n",
        "        score = np.sum(np.sqrt(np.mean(score_matrix, axis=1)))/12\n",
        "        return score, score_matrix"
      ],
      "metadata": {
        "id": "U1hc9RJNpuUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load S and W weights for WRMSSE calcualtions:\n",
        "sw_df = pd.read_pickle(INPUT_DIR_PATH_WRMSE+'sw_df.pkl')\n",
        "S = sw_df.s.values\n",
        "W = sw_df.w.values\n",
        "SW = sw_df.sw.values\n",
        "\n",
        "# Load roll up matrix to calcualte aggreagates:\n",
        "roll_mat_df = pd.read_pickle(INPUT_DIR_PATH_WRMSE+'roll_mat_df.pkl')\n",
        "roll_index = roll_mat_df.index\n",
        "roll_mat_csr = csr_matrix(roll_mat_df.values)\n",
        "del roll_mat_df"
      ],
      "metadata": {
        "id": "n1ImuAerpuXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate WRMSE score\n",
        "score = wrmsse(prediction_val.T, np.array(DF_Sales_Val.T), roll_mat_csr, S, W, SW, score_only=True)\n",
        "score"
      ],
      "metadata": {
        "id": "7-o6Nc8tpubc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "qpLEy5Tbp1tg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}